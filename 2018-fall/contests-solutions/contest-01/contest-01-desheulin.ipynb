{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузим данные:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-522338f74900>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneighbors\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKDTree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mx_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'x_train.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import KDTree\n",
    "\n",
    "x_train = np.load('x_train.npy')\n",
    "y_train = np.load('y_train.npy')\n",
    "x_test = np.load('x_test.npy')\n",
    "\n",
    "X_train = pd.DataFrame(np.load('x_train.npy'))\n",
    "X_test = pd.DataFrame(np.load('x_test.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вообще было бы неплохо разделить train сразу на train и отложенную выборку, но проще везде делать кросс-валидацию"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot some data representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairplot(df, target):\n",
    "    ncol, nrow = 7, df.shape[1] // 7 + (df.shape[1] % 7 > 0)\n",
    "    plt.figure(figsize=(ncol * 4, nrow * 4))\n",
    "\n",
    "    for i, feature in enumerate(df.columns):\n",
    "        plt.subplot(nrow, ncol, i + 1)\n",
    "        plt.scatter(df[feature], target, s=10, marker='o', alpha=.6)\n",
    "        plt.xlabel(feature)\n",
    "        if i % ncol == 0:\n",
    "            plt.ylabel('target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairplot(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все что мы нарисовали нам нужно, чтоб понять как выглядят вообще фичи, где в них категориальные, где какие и в какую сторону нужно их менять, чтотб используемые линейныйе модели давали лучший скор."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом подразделе будем применять преобрахования к данным, пожалуй это самое интересное в этом контесте.\n",
    "Давайте по пунктам:\n",
    " * Для начала вспомним как выбивали во второй домашке норм скор. Надо было применить какие-то преобразования к признакам так, чтобы получившаяся зависимость была более линейной. Сделаем тоже самое и тут. Подсказкой для этих преобразований служат названия колонок, например если признак содержит `sqft_..`, то кажется от него логичным взять корень.\n",
    " * Легко так же заметить, что `sqft_basement` = `sqft_lot` - `sqft_above`, а значит этот признак лишний и может вызывать переобучение - выкинем его. (надеюсь я то из того вычел)\n",
    " * Есть очевидно категориальные признаки, такие как `grade` или `condition`. Наверное, первое что приходит в голову, это применить [один горячий энкодер](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) прямо к ним. Но этого недостаточно. На самом деле скрывается еще один категориальный признак - `zipcodes`. Он содержит в себе информацию о районе дома. Такая информация может быть невероятно важна, когда мы оцениваем ценность дома.\n",
    " * На каком-то из меинаров говорили, что есть смысл логарифмировать целевую переменную - сделаем это. Смысл в том что предсказывать порядок модели проще, да и нет нужды по сути сказать точно последнюю цифорку, но хорошо бы точно сказать сколько нулей в стоимости.\n",
    " * Теперь, полистаем [возможные варианты](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing). Вспомним так же что во второй домашке нам показывали еще один инструмент - [PolynomialFeatures](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html#sklearn.preprocessing.PolynomialFeatures) c разными степенями. Окей, активируем его. Однако, непонятно, что делать раньше [один горячий энкодер](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) или `PolynomialFeatures`. Это вопрос возникает, когда пытаешься сделать `PolynomialFeatures` высокой степени к преобразованным признакам и ловишь `MEMory Error`. Вообще говоря, логичнее в начале применять экнодер, а потом `PolynomialFeatures`, чтоб в `PolynomialFeatures` появлялись новые осмысленные преобразования  и комбинации этого признака, однако непонятно, может он вообще не имеет смысла в комбинациях и лучше в начале применить ко всем кроме него, а его уже добавить в самом конце преобразованным. Поймем это с помощью грид серча.\n",
    " * Отлично, мы что-то сделали, но, спойлер, это привело нас только к скору ~12.5\n",
    " * У нас остались неиспользованные фичи - `lat`, `long` и первая логичная идея которая приходит в голову - добавить какую-то группировку по признакам. Эта группировка воообще говоря разумна, так как еще раз показывает модели, что квартиры в абстрактных текстильщиках, дешевле чем на абстрактной фрунзенской набережнойб хотя от центра они удалены одинаково. Можно реализовать много локальных моделей, которые будут учиться на n ближайших домах. Но это муторно. Давайте добавим в данные признак связанный со средней ценой ближайших n домов. Таким образом, мы сделаем наконец сделаем некоторое подобие [Mean Target Encoding](https://github.com/esokolov/ml-course-hse/blob/master/2018-fall/seminars/sem06.pdf)(и почему я не сделал его раньше?), добавим утечку целевой переменной и не будем с этим бороться.\n",
    " * Так скор станет близок к ~12.05, чтоб поднять его еще выше вспомним что мы делаем `PolynomialFeatures` и наш новый признак будет умножаться на всякие старые. Поэтому посчитаем не просто среднюю цену n ближайших домов, а поделим ее на разные признаки каждого из ближайших домов. То есть: $\\sum\\limits_{i=1}^n \\frac{target_i}{feature_i}$, где $n$ - количество домов. Сделаем так для `sqft_lot`, `grade`, и `condition`. Почему для них? Прост они мне понравились.\n",
    "\n",
    "На этом препроцессинг все, посмотрим как оно реализовано.\n",
    "Будьте осторожны, далее костыли и велосипеды."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LinearRegression, Lasso, LassoLars, LassoLarsCV, BayesianRidge, ElasticNet, HuberRegressor, ElasticNetCV, SGDRegressor, Ridge\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, make_scorer\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.preprocessing import RobustScaler, MinMaxScaler, PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not used due to its slowest\n",
    "# But if you like, you can ran it and wait for jne and half hour while its working\n",
    "# It's really working, I have checked\n",
    "# Thank you very much.\n",
    "\n",
    "from scipy.spatial.distance import euclidean\n",
    "class MyKDTree():\n",
    "    def __init__(self, data = None):\n",
    "        self.data = data\n",
    "    def fit(self, data):\n",
    "        self.data = data\n",
    "    def query(self, point, k = 5):\n",
    "        if self.data is None:\n",
    "            raise ValueError('Not fitted yet')\n",
    "        order = np.argsort([euclidean(data_point, point) for data_point in self.data])\n",
    "        data_sorted = self.data[order]\n",
    "        return data_sorted[:k], order[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# для группировки по 'lat' и 'long' будем использовать KDTree\n",
    "kdtree = KDTree(np.vstack([X_train['lat'], X_train['long']]).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grades_indxs = {}\n",
    "for grade in list(X_train['grade'].unique()):\n",
    "    grades_indxs[grade] = set(list(X_train.query(f'grade == {grade}').index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# было бы классно сделат класс обертку и использовать его в кросс-валидации, но нет\n",
    "def add_local_price(data, train, n_neighbours = 25):\n",
    "    # наверное возник вопрос, почему n_neighbours = 25\n",
    "    # я это подобрал на train\n",
    "    # да-да, на той же выборке на которой была утечка целевой переменной, все дела.\n",
    "    # не стоит так делать\n",
    "\n",
    "    train_data, train_labels = train\n",
    "    mean_local_price = []\n",
    "    mean_local_price_by_sqft = []    \n",
    "    mean_local_price_by_grade = []\n",
    "    mean_local_price_by_condition = []\n",
    "    for lat, long in zip(tqdm(data['lat']), data['long']):\n",
    "        point = np.array([[lat, long]])\n",
    "        # находим ближайших\n",
    "        _, ids = kdtree.query(point, k = n_neighbours)\n",
    "        ids = ids.flatten()\n",
    "        ids.sort()\n",
    "        # считаем нужные статистики\n",
    "        closest_by_sqft = train_labels[ids] / train_data.iloc[ids, train_data.columns.get_loc('sqft_lot')]\n",
    "        closest_by_grade = train_labels[ids] / train_data.iloc[ids, train_data.columns.get_loc('grade')]\n",
    "        closest_by_cond = train_labels[ids] / train_data.iloc[ids, train_data.columns.get_loc('condition')]\n",
    "        # добавляем среднее\n",
    "        mean_local_price.append(train_labels[ids].mean())\n",
    "        mean_local_price_by_sqft.append(closest_by_sqft.mean())\n",
    "        mean_local_price_by_grade.append(closest_by_grade.mean())\n",
    "        mean_local_price_by_condition.append(closest_by_cond.mean())\n",
    "    data['mean_local_price'] = mean_local_price\n",
    "    data['mean_local_price_by_sqft'] = mean_local_price_by_sqft\n",
    "    data['mean_local_price_by_grade'] = mean_local_price_by_grade\n",
    "    data['mean_local_price_by_condition'] = mean_local_price_by_condition\n",
    "\n",
    "add_local_price(X_train, train=(X_train, np.log(y_train)))\n",
    "add_local_price(X_test, train=(X_train, np.log(y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BaseEstimator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-397d9a5cd860>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# унаследуемся от всего что нужно чтоб вставлять наш класс в пайплайны\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mDataTransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseEstimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTransformerMixin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneed_one_hot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpoly_deg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoly_deg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpoly_deg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpoly_deg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BaseEstimator' is not defined"
     ]
    }
   ],
   "source": [
    "# унаследуемся от всего что нужно чтоб вставлять наш класс в пайплайны\n",
    "class DataTransform(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, need_one_hot = True, poly_deg = None, scaler = None):\n",
    "        self.poly_deg = poly_deg\n",
    "        if poly_deg is not None:\n",
    "            self.poly = PolynomialFeatures(degree = poly_deg)\n",
    "        else:\n",
    "            self.poly = PolynomialFeatures()\n",
    "        self.need_one_hot = need_one_hot\n",
    "        self.scaler = scaler\n",
    "        \n",
    "    def parser(self, data):\n",
    "        def take_year(house):\n",
    "            return (int(house.split('-')[0]) - 2014) * 365 + int(house.split('-')[1]) * 30 + int(house.split('-')[2])\n",
    "        # начнем с самого главного и полезного преобразования\n",
    "        # оно радикально подняло качество модели (нет)\n",
    "        data['waterfront'] = data['waterfront'] * 10\n",
    "        \n",
    "        # расстояние до самого дорого здания, с ним было получше\n",
    "        data['long2'] = np.abs(data['long'] - self.best_long)\n",
    "        data['lat2'] = np.abs(data['lat'] - self.best_lat)\n",
    "        \n",
    "        data['date'] = data['date'].apply(take_year)\n",
    "\n",
    "        # некоторые нелинейности\n",
    "        data['sqft_living2'] = np.sqrt(data['sqft_living'])\n",
    "        data['sqft_lot2'] = np.sqrt(data['sqft_lot'])\n",
    "        data['sqft_above2'] = np.sqrt(data['sqft_above'])\n",
    "        data['floors2'] = data['floors'] ** 2\n",
    "        data['bedrooms2'] = data['bedrooms'] ** 2\n",
    "        data['grade2'] = data['grade'] ** 2\n",
    "    \n",
    "        # насколько дачный у нас участок:\n",
    "        data['countryside_coef'] = data['sqft_living2'] / data['sqft_lot2']\n",
    "        \n",
    "        # будем считать что если дом был реконструирован, то его новая дата постройки \n",
    "        #                                         - среднее арифметическое старой и года реконструкции\n",
    "        data['yrs'] = (data['yr_built'] + data['yr_renovated'] + (data['yr_renovated'] == 0) * data['yr_built']) / 2\n",
    "        # будем считать что дома старше 1905 начинаю снова возрастать в цене\n",
    "        data['yrs'] = np.sqrt(np.abs(data['yrs'] - 1905))\n",
    "        # уберем лишнее\n",
    "        data.drop(columns=['sqft_basement', 'yr_renovated', 'yr_renovated'], inplace=True)\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.best_long = X.iloc[np.argmax(y), X.columns.get_loc('long')]\n",
    "        self.best_lat = X.iloc[np.argmax(y), X.columns.get_loc('lat')]\n",
    "        return self\n",
    "\n",
    "    def transform(self, X_):\n",
    "        X = X_.copy()\n",
    "        self.parser(X)\n",
    "        \n",
    "        # здесь очень странный код:\n",
    "        # он делает в зависимости от параметров либо в начале горячий энкодер, а потом poly, либо наоборот\n",
    "        # и иногда scaler добавляет\n",
    "        \n",
    "        if self.poly_deg is not None:\n",
    "            if self.need_one_hot:\n",
    "                df_zipcodes = pd.DataFrame(X['zipcode'])\n",
    "                df_zipcodes = pd.get_dummies(df_zipcodes, columns=['zipcode'])\n",
    "            if self.scaler is not None:\n",
    "                X = self.scaler.fit_transform(X.drop(columns = ['zipcode']))\n",
    "                X = self.poly.fit_transform(X)\n",
    "            else:\n",
    "                X = self.poly.fit_transform(X.drop(columns = ['zipcode']))\n",
    "            if self.need_one_hot:\n",
    "                X = np.hstack((X, df_zipcodes.values))\n",
    "        else:\n",
    "            if self.need_one_hot:\n",
    "                X = pd.get_dummies(X, columns=['zipcode'])\n",
    "            if self.scaler is not None:\n",
    "                X = self.scaler.fit_transform(X)\n",
    "            X = self.poly.fit_transform(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model and test it using CV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `polinomial features` and `diffrent regressions` for better result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BaseEstimator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-5d1acb3aaee0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mMix_Transformers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseEstimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTransformerMixin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'std'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         self.models = {'std': StandardScaler(),\n\u001b[1;32m      4\u001b[0m                        \u001b[0;34m'min_max'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mMinMaxScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                        'robust': RobustScaler()}\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BaseEstimator' is not defined"
     ]
    }
   ],
   "source": [
    "class Mix_Transformers(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, name = 'std'):\n",
    "        self.models = {'std': StandardScaler(),\n",
    "                       'min_max': MinMaxScaler(),\n",
    "                       'robust': RobustScaler()}\n",
    "        self.name = name\n",
    "    \n",
    "    def fit(self, X, *args, **kwargs):\n",
    "        if self.name is not None:\n",
    "            self.models[self.name].fit(np.nan_to_num(X), *args, **kwargs)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, **transform_params):\n",
    "        if self.name is not None:\n",
    "            np.nan_to_num(X, copy = False)\n",
    "            return self.models[self.name].transform(X, **transform_params)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-c2ff9af250b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m full_lasso = Pipeline([\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0;34m'transf'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataTransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0;34m'scalers'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMix_Transformers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0;34m'estimator'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLasso\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m ])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "full_lasso = Pipeline([\n",
    "    ('transf', DataTransform()),\n",
    "    ('scalers', Mix_Transformers()),\n",
    "    ('estimator', Lasso())\n",
    "])\n",
    "\n",
    "full_ridge = Pipeline([\n",
    "    ('transf', DataTransform()),\n",
    "    ('scalers', Mix_Transformers()),\n",
    "    ('estimator', Ridge())\n",
    "])\n",
    "\n",
    "full_HR = Pipeline([\n",
    "    ('transf', DataTransform()),\n",
    "    ('scalers', Mix_Transformers()),\n",
    "    ('estimator', HuberRegressor())\n",
    "])\n",
    "\n",
    "# Таких было очень много, буквально для каждого эстиматора\n",
    "full_grid_HR = {'estimator__alpha': np.logspace(-1, 4, 10), 'estimator__max_iter' : [1500],\n",
    "             'scalers__name' : ['std', 'min_max'], 'transf__poly_deg' : [None, 3], 'transf__scaler' : [None]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metric from contest:\n",
    "def mape(y_true, y_pred): \n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GridSearchCV' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-1d727a57de83>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# пример такого грид серча:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgrid_HR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_HR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_grid_HR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmake_scorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgreater_is_better\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mgrid_HR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mgrid_HR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrid_HR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'GridSearchCV' is not defined"
     ]
    }
   ],
   "source": [
    "# пример такого грид серча:\n",
    "grid_HR = GridSearchCV(full_HR, full_grid_HR, scoring=make_scorer(mape, greater_is_better=False),n_jobs=-1, verbose=2)\n",
    "grid_HR.fit(X_train, y_train)\n",
    "grid_HR.best_score_, grid_HR.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'make_pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-cb4902708682>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# из них были выбраны наиболее мощные:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlasso\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDataTransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLasso\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mENet2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDataTransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mElasticNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.00035\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1700\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mENet3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDataTransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mElasticNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1700\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mHR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDataTransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHuberRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1700\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'make_pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "# из них были выбраны наиболее мощные:\n",
    "lasso = make_pipeline(DataTransform(), StandardScaler(), Lasso(alpha=1e-3, max_iter=5000))\n",
    "ENet2 = make_pipeline(DataTransform(), StandardScaler(), ElasticNet(alpha=0.00035, max_iter=1700))\n",
    "ENet3 = make_pipeline(DataTransform(), StandardScaler(), ElasticNet(alpha=1e-3, max_iter=1700))\n",
    "HR = make_pipeline(DataTransform(), StandardScaler(), HuberRegressor(alpha=1000, max_iter=1700))\n",
    "HR3 = make_pipeline(DataTransform(), StandardScaler(), HuberRegressor(alpha=1200, max_iter=1700))\n",
    "ridge = make_pipeline(DataTransform(poly_deg=3), StandardScaler(), Ridge(alpha=1200, max_iter=1700))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply our model and save result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saver(path, res):\n",
    "    with open(path, 'w') as out:\n",
    "        print('Id,Price', file = out)\n",
    "        for i in range(len(res)):\n",
    "            print('{i},{res}'.format(i = i + 1, res=res[i]), file = out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'tqdm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-c7f24ec3e3bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# new ultra top method for averaging results of diffrent models:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# import asyncio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# чтоб еще больше поднять резалт, возьмем среднее предсказание по лучшим моделям\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'tqdm'"
     ]
    }
   ],
   "source": [
    "# new ultra top method for averaging results of diffrent models:\n",
    "# import asyncio\n",
    "from tqdm import tqdm\n",
    "\n",
    "# чтоб еще больше поднять резалт, возьмем среднее предсказание по лучшим моделям\n",
    "# снова унаследуемся от всех\n",
    "\n",
    "# source: https://www.kaggle.com/alexpengxiao/preprocessing-model-averaging-by-xgb-lgb-1-39\n",
    "class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n",
    "    def __init__(self, models):\n",
    "        self.models = models\n",
    "        \n",
    "    # we define clones of the original models to fit the data in\n",
    "    def fit(self, X, y):\n",
    "        self.models_ = [clone(x) for x in self.models]\n",
    "        #loop = asyncio.get_event_loop()\n",
    "        \n",
    "        # Train cloned base models\n",
    "        for model in tqdm(self.models_):\n",
    "            model.fit(X, y)\n",
    "\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    #Now we do the predictions for cloned models and average them\n",
    "    def predict(self, X):\n",
    "        predictions = np.column_stack([\n",
    "            model.predict(X) for model in tqdm(self.models_)\n",
    "        ])\n",
    "        return np.mean(predictions, axis=1)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-703f33d9f215>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0maveraged_models\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAveragingModels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mHR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mENet2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHR3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mENet3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mLR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maveraged_models\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# заметим, что метрика из контеста несимметрична и штрафует за ошибки ввверх более чем за ошибки вниз\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_train' is not defined"
     ]
    }
   ],
   "source": [
    "y_train = np.log(y_train)\n",
    "averaged_models = AveragingModels(models = (HR, ENet2, HR3, ENet3))\n",
    "LR = averaged_models.fit(X_train, y_train)\n",
    "res = LR.predict(X_test)\n",
    "# заметим, что метрика из контеста несимметрична и штрафует за ошибки ввверх более чем за ошибки вниз\n",
    "# воспользуемся этим:\n",
    "res = np.exp(res) * 0.99\n",
    "saver('res_weighted_averaged_8.csv', res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
